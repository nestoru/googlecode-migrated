= Amazon Feeds Parser =
The inspiration for this project was a need to parse big Amazon feeds XML files (dozens of GB) in an acceptable time.

After giving up with XSLT I decided to give "xgawk" a try.
       
== The project ==
The project is hosted at http://nestorurquiza.googlecode.com/svn/trunk/amazon-feeds/
This is an early phase and by the time of this writing it is composed of only three xgawk files, one perl script to manage processing of multiple feeds concurrently and some  chunks from the big Amazon feeds like for example the biggest one "Apparel" root Category:
{{{
.
|-- amz_categories.xgawk
|-- amz_product_category.xgawk
|-- amz_products.xgawk
|-- amz_xml_to_tsv.pl
|-- output
`-- xml-sources
    |-- categories
    |-- chunk_us_ecs_apparel.xml
    |-- chunk_us_ecs_cellphone.xml
    |-- chunk_us_ecs_watches.xml
    |-- product_category
    `-- products
}}}

To process individual feeds use:
{{{
xgawk -f amz_categories.xgawk us_ecs_apparel.xml > amz_categories
xgawk -f amz_products.xgawk us_ecs_apparel.xml > amz_products
xgawk -f amz_product_category.xgawk us_ecs_apparel.xml > amz_product_category
}}}
You will end up with three tab separated values (tsv) files that should be enough to start rendering and searching Amazon products and nodes (or categories).

Of course you need to have xgawk ready. Below are the steps I followed in my local MacBook Pro (2.26 GHz Intel Core 2 Duo, 4GB 1067 MHz DDR3)

 1. Download the sources from the xgawk official site.
 1. Uncompress them. (in my case xgawk-3.1.6a-20090408)
 1. Run the below commands:
{{{
cd xgawk-3.1.6a-20090408/
./configure 
make
sudo make install
}}}
 1. Add the below line to file ~/.bash_profile
{{{
export PATH=/opt/local/bin:/opt/local/sbin:$PATH
}}}

To process several feeds use the perl script with no parameters. Note you need to change the $PROJECT_HOME variable inside the script to point to your local project base directory. 

After you run it you will get some data below the output directory. All ".tsv" files are created directly from each root category feed xml file. 
{{{
-- output
|   |-- chunk_us_ecs_apparel_categories.tsv
|   |-- chunk_us_ecs_apparel_product_category.tsv
|   |-- chunk_us_ecs_apparel_products.tsv
|   |-- chunk_us_ecs_cellphone_categories.tsv
|   |-- chunk_us_ecs_cellphone_product_category.tsv
|   |-- chunk_us_ecs_cellphone_products.tsv
|   |-- chunk_us_ecs_watches_categories.tsv
|   |-- chunk_us_ecs_watches_product_category.tsv
|   `-- chunk_us_ecs_watches_products.tsv
}}}

Even though the included fields are a subset of all available you can edit the corresponding xgawk file to include more or delete some of them.

Now you can import the files in your local DB Engine.

== Performance ==
Below are some numbers from my local environment. I have to say that by the time the scripts were running I was writing emails, browsing the web, programming in Perl, awk and Java, even chatting on skype and msn. I did not experience any big delays on any of those tasks but of course they might have affected the below results. I might come back and post some results from our Red Hat Servers when the package is deployed there.

The file parsed here (us_ecs_apparel.xml) is 34GB. I got:
{{{
       72min for 175033 products (id, title, price, description) resulting in 20MB of tsv file.
       74min for 7866 nodes/categories (id, parent, name) resulting in 223KB of tsv file. 
       74min for 965733 product-nodes/product_categories (product_id, node_id) mappings resulting in 19MB of tsv file.
}}}

After I added image, height and width fields for small, medium and large images (total 9 fields) the processing time increased considerably:
{{{
       140min for 175033 products (id, title, price, description, lg_img, md_img, sm_img, lg_height, md_height, sm_height, lg_width, md_width, sm_width) resulting in 75MB of tsv file.
}}} 

You can get the total execution time per script while commenting out the below line:
{{{
#print "Took me " systime()-start_time " seconds"
}}}

Memory footprint was very slow (not even 2MB of physical and half GB virtual memory). they were processor intensive tasks of course.

The more challenging code to write was the script amz_categories.xgawk as combinations of nodes and parents are repeated again and again per product. The technique I have used is just inserting into a two-dimensional array (which in awk language can be seen actually as a two-dimensional hash) both child and parent as keys. That is the reason why memory will be used and no output from the command will be available up to the moment it finishes running.

== Contributing ==
Please report any bugs or contributions to nestor dot urquiza at gmail dot com.